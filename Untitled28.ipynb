{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MWtB_gVY7cy"
      },
      "outputs": [],
      "source": [
        "#1st cloud function\n",
        "import pandas as pd\n",
        "import vertexai\n",
        "import time\n",
        "import tqdm\n",
        "from typing import Optional, Sequence\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud import documentai\n",
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "\n",
        "\n",
        "def chunking_and_embedding(request):\n",
        "  #declare the required variables\n",
        "  project_id = \"Enter your project id\"\n",
        "  location = \"us\" # Format is \"us\" or \"eu\"\n",
        "  processor_id = \"2fa558c697497e1b\" # Create processor before running sample\n",
        "  processor_version = \"pretrained-ocr-v2.0-2023-06-02\" # Refer to https://cloud.google.com/document-ai/docs/manage-processor-versions for more information\n",
        "  mime_type = \"application/pdf\"\n",
        "\n",
        "  #extract file_path from the request\n",
        "  request_json = request.get_json(silent=True)\n",
        "  request_args = request.args\n",
        "  file_path = \"/content/2023-annual-report-1-15 (1).pdf\"\n",
        "  if request_json and \"file_path\" in request_json:\n",
        "    file_path = request_json[\"file_path\"]\n",
        "  elif request_args and \"file_path\" in request_args:\n",
        "    file_path = request_args[\"file_path\"]\n",
        "  else:\n",
        "    #throw error saying file path missing\n",
        "    return \"Error: Missing FilePath\"\n",
        "\n",
        "  process_options = documentai.ProcessOptions(\n",
        "    ocr_config=documentai.OcrConfig(\n",
        "      enable_native_pdf_parsing=True,\n",
        "      enable_image_quality_scores=True,\n",
        "      enable_symbol=True,\n",
        "      # OCR Add Ons https://cloud.google.com/document-ai/docs/ocr-add-ons\n",
        "      premium_features=documentai.OcrConfig.PremiumFeatures(\n",
        "          compute_style_info=True,\n",
        "          enable_math_ocr=False,  # Enable to use Math OCR Model\n",
        "          enable_selection_mark_detection=True,\n",
        "      ),\n",
        "    )\n",
        "  )\n",
        "\n",
        "  client = documentai.DocumentProcessorServiceClient(\n",
        "    client_options=ClientOptions(\n",
        "      api_endpoint=f\"{location}-documentai.googleapis.com\"\n",
        "    )\n",
        "  )\n",
        "  name = client.processor_version_path(\n",
        "    project_id, location, processor_id, processor_version\n",
        "  )\n",
        "\n",
        "  # Read the file into memory\n",
        "  with open(file_path, \"rb\") as image:\n",
        "    image_content = image.read()\n",
        "\n",
        "  # Configure the process request\n",
        "  request = documentai.ProcessRequest(\n",
        "    name=name,\n",
        "    raw_document=documentai.RawDocument(content=image_content, mime_type=mime_type),\n",
        "    # Only supported for Document OCR processor\n",
        "    process_options=process_options,\n",
        "  )\n",
        "  result = client.process_document(request=request)\n",
        "  document = result.document\n",
        "\n",
        "  text = document.text\n",
        "  para = []\n",
        "\n",
        "  #for converting the chunks\n",
        "  def layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\n",
        "    \"\"\"\n",
        "    Document AI identifies text in different parts of the document by their\n",
        "    offsets in the entirety of the document\"s text. This function converts\n",
        "    offsets to a string.\n",
        "    \"\"\"\n",
        "    # If a text segment spans several lines, it will\n",
        "    # be stored in different text segments.\n",
        "    return \"\".join(\n",
        "        text[int(segment.start_index) : int(segment.end_index)]\n",
        "        for segment in layout.text_anchor.text_segments\n",
        "    )\n",
        "  for page in document.pages:\n",
        "    for p in page.paragraphs:\n",
        "      t = layout_to_text(p.layout,text)\n",
        "      para.append(t.replace(\"\\n\", \"\"))\n",
        "\n",
        "  #create a dataframe and store the data\n",
        "  d = {\"title\":text}\n",
        "  df = pd.DataFrame.from_dict(d)\n",
        "  df[\"id\"] = df.index + 1\n",
        "\n",
        "  #for embedding generation\n",
        "  vertexai.init(project=project_id, location=location)\n",
        "  model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
        "  BATCH_SIZE = 1\n",
        "  embs = []\n",
        "  def get_embeddings_wrapper(texts):\n",
        "    embs = []\n",
        "    for i in tqdm.tqdm(range(0, len(texts), BATCH_SIZE)):\n",
        "        time.sleep(1)  # to avoid the quota error\n",
        "        result = model.get_embeddings(texts[i : i + BATCH_SIZE])\n",
        "        embs = embs + [e.values for e in result]\n",
        "    return embs\n",
        "  df = df.assign(embedding=get_embeddings_wrapper(list(df.title)))\n",
        "\n",
        "  #returning head of df as response\n",
        "  return df.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#2nd cloud function\n",
        "import numpy as np\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from google.cloud import discoveryengine_v1alpha as discoveryengine\n",
        "\n",
        "\n",
        "def inferencing(request):\n",
        "  project_id = PROJECT_ID\n",
        "  #extract prompt from the request\n",
        "  request_json = request.get_json(silent=True)\n",
        "  request_args = request.args\n",
        "\n",
        "  prompt = \"\"\n",
        "  if request_json and \"prompt\" in request_json:\n",
        "    file_path = request_json[\"prompt\"]\n",
        "  elif request_args and \"prompt\" in request_args:\n",
        "    file_path = request_args[\"prompt\"]\n",
        "  else:\n",
        "    #throw error saying file path missing\n",
        "    return \"Error: Missing Prompt\"\n",
        "\n",
        "  #extract deployed index id from request\n",
        "  DEPLOYED_INDEX_ID = \"\"\n",
        "  if request_json and \"DEPLOYED_INDEX_ID\" in request_json:\n",
        "    DEPLOYED_INDEX_ID = request_json[\"DEPLOYED_INDEX_ID\"]\n",
        "  elif request_args and \"DEPLOYED_INDEX_ID\" in request_args:\n",
        "    DEPLOYED_INDEX_ID = request_args[\"DEPLOYED_INDEX_ID\"]\n",
        "  else:\n",
        "    #throw error saying DEPLOYED_INDEX_ID missing\n",
        "    return \"Error: Missing DEPLOYED_INDEX_ID\"\n",
        "\n",
        "  test_embeddings = get_embeddings_wrapper([prompt])\n",
        "  content = []\n",
        "\n",
        "  #query the index\n",
        "  response = my_index_endpoint.find_neighbors(\n",
        "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
        "    queries=test_embeddings,\n",
        "    num_neighbors=20,\n",
        "    )\n",
        "  cnt=0\n",
        "  for idx, neighbor in enumerate(response[0]):\n",
        "    id = np.int64(neighbor.id)\n",
        "    similar = df.query(\"id == @id\", engine=\"python\")\n",
        "    cnt+=1\n",
        "    content.append(similar.title.values[0])\n",
        "  #summarization using gemini\n",
        "  query = f'''\n",
        "  {content[0]}\n",
        "  {content[1]}\n",
        "  {content[2]}\n",
        "  {content[3]}\n",
        "  {content[4]}\n",
        "  Summarize the above points.\n",
        "  '''\n",
        "  vertexai.init(project=project_id, location=\"us-central1\")\n",
        "  model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "  response = model.generate_content(query)\n",
        "\n",
        "  return response.text\n"
      ]
    }
  ]
}
